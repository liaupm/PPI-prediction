{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5038b5-b488-4b79-a403-c429a0ea885f",
   "metadata": {},
   "source": [
    "Tras haber visto como estabilizar la función de atención, voy a intentar comprobar si los einsums están haciendola mucho más lenta que sumas y multiplicaciones normales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814f04b3-1466-4a4f-a712-357b48b29192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61023204-acfe-4de2-af46-9d3faa93ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable multihead attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, hidden_size, head_size, num_heads):\n",
    "    super().__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.head_size = head_size\n",
    "    self.num_heads = num_heads\n",
    "\n",
    "    self.w_u = nn.Linear(hidden_size, head_size * num_heads)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, L, H = x.size()\n",
    "    u = self.w_u(x)\n",
    "    u = u.view(B, L, self.num_heads, self.head_size) # [B, L, n, Dh]\n",
    "\n",
    "    A = torch.einsum(\"blnd,bknd->blnk\", u, u) # Attention matrices [B, L, n, L]\n",
    "    A = A.contiguous() # important since einsum leaves A discontigous so view cannot be used\n",
    "\n",
    "    # Substract the max value for each batch example to prevent overflows in the exp (stabilization).\n",
    "    A = A.view(B, L, self.num_heads*L) - A.view(B, -1).max(dim=1, keepdim=True).values.unsqueeze(-1)\n",
    "    exp_A = torch.exp(A)\n",
    "    \n",
    "    alpha = torch.einsum(\"bln->bl\",exp_A) / torch.einsum(\"bln->b\", exp_A).unsqueeze(-1)\n",
    "\n",
    "    attended = torch.einsum(\"bl,blh->bh\", alpha, x)\n",
    "    return attended\n",
    "\n",
    "# Stable multihead attention\n",
    "class FastMultiHeadAttention(nn.Module):\n",
    "  def __init__(self, hidden_size, head_size, num_heads):\n",
    "    super().__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.head_size = head_size\n",
    "    self.num_heads = num_heads\n",
    "\n",
    "    self.w_u = nn.Linear(hidden_size, head_size * num_heads)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, L, H = x.size()\n",
    "    u = self.w_u(x)\n",
    "    u = u.view(B, L, self.num_heads, self.head_size) # [B, L, n, Dh]\n",
    "    \n",
    "    #  A = torch.einsum(\"blnd,bknd->blnk\", u, u)\n",
    "    u = u.permute(0, 2, 1, 3)\n",
    "    A = torch.matmul(u, u.mT).permute(0, 2, 1, 3)\n",
    "    A = A.contiguous()\n",
    "    # Substract the max value for each batch example to prevent overflows in the exp (stabilization).\n",
    "    A = A.view(B, L, self.num_heads*L) - A.view(B, -1).max(dim=1, keepdim=True).values.unsqueeze(-1)\n",
    "    exp_A = torch.exp(A)\n",
    "    \n",
    "    alpha = torch.sum(exp_A, dim=-1) / torch.sum(exp_A.view(B, -1), dim=-1, keepdim=True)\n",
    "\n",
    "    # attended = torch.einsum(\"bl,blh->bh\", alpha, x)\n",
    "    attended = torch.matmul(alpha.unsqueeze(1), x).squeeze(1)\n",
    "    return attended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "685a304e-a920-4630-8624-c809fb0f65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "slowma = torch.compile(MultiHeadAttention(1024, 32, 2).to(\"cuda\"))\n",
    "fastma = torch.compile(FastMultiHeadAttention(1024, 32, 2).to(\"cuda\"))\n",
    "x = torch.randn(32, 1000, 1024).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af34a52-7d62-4d24-9643-2a41b4a2c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "import time\n",
    "\n",
    "@register_cell_magic\n",
    "def timercell(line, cell):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    exec(cell, globals())\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"⏱️ Elapsed: {start.elapsed_time(end) / 1000} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed251868-8007-44c7-b322-fb179653a568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Elapsed: 13.7028505859375 s\n"
     ]
    }
   ],
   "source": [
    "%%timercell\n",
    "with torch.no_grad():\n",
    "    for i in range(10000):\n",
    "        slowma(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dd1613b-8649-4f5e-aa09-845ccbf77bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Elapsed: 13.560544921875 s\n"
     ]
    }
   ],
   "source": [
    "%%timercell\n",
    "with torch.no_grad():\n",
    "    for i in range(10000):\n",
    "        fastma(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52527f5-df96-4671-a2a3-5ea08c7f89ae",
   "metadata": {},
   "source": [
    "It seems like there is no much difference between the two approaches. I will stick with einsum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eee8e08-9cd6-470b-858f-0e72fdaad969",
   "metadata": {},
   "outputs": [],
   "source": [
    "slowma = MultiHeadAttention(1024, 32, 2).to(\"cuda\")\n",
    "fastma = FastMultiHeadAttention(1024, 32, 2).to(\"cuda\")\n",
    "x = torch.randn(32, 1000, 1024).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53516229-198b-4921-a7f5-7289630bbd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Elapsed: 26.742609375 s\n"
     ]
    }
   ],
   "source": [
    "%%timercell\n",
    "with torch.no_grad():\n",
    "    for i in range(10000):\n",
    "        slowma(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504894b5-da14-41c7-8ec1-659141522411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Elapsed: 26.7568828125 s\n"
     ]
    }
   ],
   "source": [
    "%%timercell\n",
    "with torch.no_grad():\n",
    "    for i in range(10000):\n",
    "        fastma(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008ba1e6-5815-412e-b086-371b3a1ab67d",
   "metadata": {},
   "source": [
    "Neither does it when they are not compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "667165b4-0054-4795-b6e3-004e7677801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This torch option seems to greatly improve performance when compiling:\n",
    "#torch.set_float32_matmul_precision('high')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
