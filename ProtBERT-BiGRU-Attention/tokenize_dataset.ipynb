{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33317d1d-c3ba-47d6-8f8f-c170a5239f3c",
   "metadata": {},
   "source": [
    "# Train ProtBERT-GRU-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a941b-e933-4596-9cfe-6459d7e4ea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 10:32:47.384178: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-26 10:32:47.387791: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-26 10:32:47.455937: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-26 10:32:48.202501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LOCATION = Path(\"../datasets/bernett_v4/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab9ece-9041-4854-981f-5127e0de8ebf",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51c654a-4df2-4ef2-af77-4e4268afecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8160 positives, 8160 negatives, 16320 total.\n",
      "Validation size: 2963 positives, 2963 negatives, 5926 total.\n",
      "Test size: 2602 positives, 2602 negatives, 5204 total.\n"
     ]
    }
   ],
   "source": [
    "train_pos = pd.read_csv(\"bernett/TrainPPI/Intra1_pos_rr.txt\", sep=\" \", header=None)\n",
    "train_neg = pd.read_csv(\"bernett/TrainPPI/Intra1_neg_rr.txt\", sep=\" \", header=None)\n",
    "val_pos = pd.read_csv(\"bernett/TrainPPI/Intra0_pos_rr.txt\", sep=\" \", header=None)\n",
    "val_neg = pd.read_csv(\"bernett/TrainPPI/Intra0_neg_rr.txt\", sep=\" \", header=None)\n",
    "test_pos = pd.read_csv(\"bernett/TrainPPI/Intra2_pos_rr.txt\", sep=\" \", header=None)\n",
    "test_neg = pd.read_csv(\"bernett/TrainPPI/Intra2_neg_rr.txt\", sep=\" \", header=None)\n",
    "\n",
    "print(f\"Train size: {train_pos.shape[0]} positives, {train_neg.shape[0]} negatives, {train_pos.shape[0] + train_neg.shape[0]} total.\")\n",
    "print(f\"Validation size: {val_pos.shape[0]} positives, {val_neg.shape[0]} negatives, {val_pos.shape[0] + val_neg.shape[0]} total.\")\n",
    "print(f\"Test size: {test_pos.shape[0]} positives, {test_neg.shape[0]} negatives, {test_pos.shape[0] + test_neg.shape[0]} total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08725d07-c944-4e1d-ae8c-35266cd46496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_check</th>\n",
       "      <th>nan_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_pos</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_neg</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_pos</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_neg</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_pos</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_neg</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           shape_check  nan_check\n",
       "train_pos         True       True\n",
       "train_neg         True       True\n",
       "val_pos           True       True\n",
       "val_neg           True       True\n",
       "test_pos          True       True\n",
       "test_neg          True       True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape and NaN values\n",
    "dfs = [train_pos, train_neg, val_pos, val_neg, test_pos, test_neg]\n",
    "results = {\n",
    "    \"shape_check\": [df.shape[1] == 2 for df in dfs],\n",
    "    \"nan_check\": [df.isna().sum().sum() == 0 for df in dfs]\n",
    "}\n",
    "results = pd.DataFrame(results, index=[\"train_pos\", \"train_neg\", \"val_pos\", \"val_neg\", \"test_pos\", \"test_neg\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623292a5-50dd-46d3-bc20-b0577fb1d4b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m seq_dict \u001b[38;5;241m=\u001b[39m {record\u001b[38;5;241m.\u001b[39mid: \u001b[38;5;28mstr\u001b[39m(record\u001b[38;5;241m.\u001b[39mseq) \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m SeqIO\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbernett/human_swissprot_oneliner.fasta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfasta\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[1;32m      6\u001b[0m mapped_dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdfs\u001b[49m):\n\u001b[1;32m      8\u001b[0m   df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapplymap(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mid\u001b[39m: seq_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mid\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan))\n\u001b[1;32m      9\u001b[0m   df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "# Load your FASTA file and create a mapping of ID -> Sequence\n",
    "seq_dict = {record.id: str(record.seq) for record in SeqIO.parse(DATASET_LOCATION / \"human_swissprot_oneliner.fasta\", \"fasta\")}\n",
    "\n",
    "mapped_dfs = []\n",
    "for i, df in enumerate(dfs):\n",
    "  df = df.applymap(lambda id: seq_dict.get(id, np.nan))\n",
    "  df.columns = ['seq1', 'seq2']\n",
    "  df['label'] = (i+1) % 2\n",
    "  mapped_dfs.append(df)\n",
    "\n",
    "results = {\n",
    "    \"shape_check\": [df.shape[1] == 3 for df in mapped_dfs],\n",
    "    \"nan_check\": [df.isna().sum().sum() for df in mapped_dfs]\n",
    "}\n",
    "results = pd.DataFrame(results, index=[\"train_pos\", \"train_neg\", \"val_pos\", \"val_neg\", \"test_pos\", \"test_neg\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf56a0b5-c321-4edd-a4e1-d2c91d09e8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['seq1', 'seq2', 'label'],\n",
       "        num_rows: 16320\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['seq1', 'seq2', 'label'],\n",
       "        num_rows: 5926\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['seq1', 'seq2', 'label'],\n",
       "        num_rows: 5204\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_dataset = DatasetDict({\n",
    "  \"train\": Dataset.from_pandas(pd.concat([mapped_dfs[0], mapped_dfs[1]]).reset_index(drop=True)),\n",
    "  \"validation\": Dataset.from_pandas(pd.concat([mapped_dfs[2], mapped_dfs[3]]).reset_index(drop=True)),\n",
    "  \"test\": Dataset.from_pandas(pd.concat([mapped_dfs[4], mapped_dfs[5]]).reset_index(drop=True))\n",
    "})\n",
    "seq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21c9a5f5-151a-4e14-af0e-d562c950f9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([8160, 8160]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(seq_dataset[\"train\"][\"label\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d603809-6ea9-4970-943b-c8bf7ad2b3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2963, 2963]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(seq_dataset[\"validation\"][\"label\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "671537df-1fa0-43ea-aaf8-1ba2a22a0298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2602, 2602]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(seq_dataset[\"test\"][\"label\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f75de-995a-4cd6-bbd3-8d809eb8a0c0",
   "metadata": {},
   "source": [
    "The dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88ce7a0c-2b8b-4bd2-b1e1-e215c3541895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free some memory\n",
    "del mapped_dfs, dfs, results, seq_dict, train_pos, train_neg, val_pos, val_neg, test_pos, test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1caa7-d196-47d3-be09-5bf6dcd4aa80",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49abbf0d-950d-4e40-8c86-11cd76d9d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05625932-f9f9-4bb6-803c-57543b3e761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "def tokenize(batch, tokenizer, N=500):\n",
    "  seqs1, seqs2 = batch['seq1'], batch['seq2'] # batch must be a dataframe with two columns.\n",
    "\n",
    "  # replace unknown aminoacids and turn to uppercase\n",
    "  seqs1 = [re.sub(r\"[UZOB]\", \"X\", seq.upper()) for seq in seqs1]\n",
    "  seqs2 = [re.sub(r\"[UZOB]\", \"X\", seq.upper()) for seq in seqs2]\n",
    "\n",
    "  # Truncate sequences according to the paper\n",
    "  seqs1 = [seq[:math.ceil(N/2)] + seq[-N//2:] if len(seq) > N else seq for seq in seqs1]\n",
    "  seqs2 = [seq[:math.ceil(N/2)] + seq[-N//2:] if len(seq) > N else seq for seq in seqs2]\n",
    "\n",
    "  # introduce spaces between sequences and pad those that are not at least 500\n",
    "  seqs1 = [' '.join(seq) + f\" {tokenizer.pad_token}\"*(N - len(seq)) for seq in seqs1]\n",
    "  seqs2 = [' '.join(seq) + f\" {tokenizer.pad_token}\"*(N - len(seq)) for seq in seqs2]\n",
    "\n",
    "  return tokenizer(seqs1, seqs2, padding=False, truncation=False)\n",
    "\n",
    "tokenize_batch = partial(tokenize, tokenizer=tokenizer, N=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc85f10-1ea8-4859-b6ec-06880a12b12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568bf87b40324f5d967cd05664487e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac2c441595642bf860b056080135c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfb7fd847c54d89a486eaf7b4748513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_dataset = seq_dataset.map(tokenize_batch, num_proc=1, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "162b3280-4a18-4b06-b48c-a1c1e20e9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_dataset['train'] = tok_dataset['train'].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "279f8752-58b2-47a9-92e8-bc3d1e25c1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_dataset['train']['label'][10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aba330d9-6374-4d06-a7a2-4996425fe25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b963e90b4c4cc5a52039f67e7621e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/16320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b987c1eb044b9c919a67be1ded23ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d227b77ff84c16b44bb99c0ffbae1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_dataset.save_to_disk(\"tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fde99c96-c8e6-4985-9025-8a90bab39330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['seq1', 'seq2', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 16320\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['seq1', 'seq2', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5926\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['seq1', 'seq2', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5204\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d72c50-0015-487f-b56b-b59b34f8778e",
   "metadata": {},
   "source": [
    "# Full tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003fdb7d-996c-40d0-a1ec-c7dae9e089ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 81596 positives, 81596 negatives, 163192 total.\n",
      "Validation size: 29630 positives, 29630 negatives, 59260 total.\n",
      "Test size: 26024 positives, 26024 negatives, 52048 total.\n"
     ]
    }
   ],
   "source": [
    "train_pos = pd.read_csv(DATASET_LOCATION / \"Intra1_pos_rr.txt\", sep=\" \", header=None)\n",
    "train_neg = pd.read_csv(DATASET_LOCATION / \"Intra1_neg_rr.txt\", sep=\" \", header=None)\n",
    "val_pos = pd.read_csv(DATASET_LOCATION / \"Intra0_pos_rr.txt\", sep=\" \", header=None)\n",
    "val_neg = pd.read_csv(DATASET_LOCATION / \"Intra0_neg_rr.txt\", sep=\" \", header=None)\n",
    "test_pos = pd.read_csv(DATASET_LOCATION / \"Intra2_pos_rr.txt\", sep=\" \", header=None)\n",
    "test_neg = pd.read_csv(DATASET_LOCATION / \"Intra2_neg_rr.txt\", sep=\" \", header=None)\n",
    "\n",
    "print(f\"Train size: {train_pos.shape[0]} positives, {train_neg.shape[0]} negatives, {train_pos.shape[0] + train_neg.shape[0]} total.\")\n",
    "print(f\"Validation size: {val_pos.shape[0]} positives, {val_neg.shape[0]} negatives, {val_pos.shape[0] + val_neg.shape[0]} total.\")\n",
    "print(f\"Test size: {test_pos.shape[0]} positives, {test_neg.shape[0]} negatives, {test_pos.shape[0] + test_neg.shape[0]} total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5ebdc75-0064-48dd-8c6a-2d3461bbbd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_check</th>\n",
       "      <th>nan_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_pos</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_neg</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_pos</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_neg</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_pos</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_neg</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           shape_check  nan_check\n",
       "train_pos         True       True\n",
       "train_neg         True       True\n",
       "val_pos           True       True\n",
       "val_neg           True       True\n",
       "test_pos          True       True\n",
       "test_neg          True       True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape and NaN values\n",
    "dfs = [train_pos, train_neg, val_pos, val_neg, test_pos, test_neg]\n",
    "results = {\n",
    "    \"shape_check\": [df.shape[1] == 2 for df in dfs],\n",
    "    \"nan_check\": [df.isna().sum().sum() == 0 for df in dfs]\n",
    "}\n",
    "results = pd.DataFrame(results, index=[\"train_pos\", \"train_neg\", \"val_pos\", \"val_neg\", \"test_pos\", \"test_neg\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e88bb72-f74b-4fbf-80f4-21e90d06076a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_check</th>\n",
       "      <th>nan_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_pos</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_neg</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_pos</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_neg</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_pos</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_neg</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           shape_check  nan_check\n",
       "train_pos         True          0\n",
       "train_neg         True          0\n",
       "val_pos           True          0\n",
       "val_neg           True          0\n",
       "test_pos          True          0\n",
       "test_neg          True          0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "# Load your FASTA file and create a mapping of ID -> Sequence\n",
    "seq_dict = {record.id: str(record.seq) for record in SeqIO.parse(\"bernett/human_swissprot_oneliner.fasta\", \"fasta\")}\n",
    "\n",
    "mapped_dfs = []\n",
    "for i, df in enumerate(dfs):\n",
    "  df = df.applymap(lambda id: seq_dict.get(id, np.nan))\n",
    "  df.columns = ['seq1', 'seq2']\n",
    "  df['label'] = (i+1) % 2\n",
    "  mapped_dfs.append(df)\n",
    "\n",
    "results = {\n",
    "    \"shape_check\": [df.shape[1] == 3 for df in mapped_dfs],\n",
    "    \"nan_check\": [df.isna().sum().sum() for df in mapped_dfs]\n",
    "}\n",
    "results = pd.DataFrame(results, index=[\"train_pos\", \"train_neg\", \"val_pos\", \"val_neg\", \"test_pos\", \"test_neg\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f79e43e2-890a-4de3-b168-fe7ab6c2b083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['seq1', 'seq2', 'label'],\n",
       "        num_rows: 163192\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['seq1', 'seq2', 'label'],\n",
       "        num_rows: 59260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['seq1', 'seq2', 'label'],\n",
       "        num_rows: 52048\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_dataset = DatasetDict({\n",
    "  \"train\": Dataset.from_pandas(pd.concat([mapped_dfs[0], mapped_dfs[1]]).reset_index(drop=True)),\n",
    "  \"validation\": Dataset.from_pandas(pd.concat([mapped_dfs[2], mapped_dfs[3]]).reset_index(drop=True)),\n",
    "  \"test\": Dataset.from_pandas(pd.concat([mapped_dfs[4], mapped_dfs[5]]).reset_index(drop=True))\n",
    "})\n",
    "seq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6485a566-3023-45d2-b7c9-0bb7ebad20dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([81596, 81596]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(seq_dataset[\"train\"][\"label\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac11cd79-4520-4f6f-824b-f0a2f74b46c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([29630, 29630]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(seq_dataset[\"validation\"][\"label\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62fdd14f-a991-4971-bdea-d2a5a2c765fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([26024, 26024]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(seq_dataset[\"test\"][\"label\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa6fcdf4-bae0-4e1f-8c5a-d9e4f91fdb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f2b9492b994bd9b3bc25ef8aa6c812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/163192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37990d9844c455783e90d165924dae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1b61fb4c3441febbf3e65c04caaa8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_dataset = seq_dataset.map(tokenize_batch, num_proc=1, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48e2396d-0a72-41ab-a83b-c280a4c0b8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0d6c00b4284b42a3c02d3992ec1b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/163192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4051752abcf4c6d97e671908e731fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/59260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42080d130360449a863b04ae2bb3fc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/52048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_dataset.save_to_disk(\"tokenized_dataset_full\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
